{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0 - legitimate\n",
    "# 1 - phishing\n",
    "# 2 - suspicious\n",
    "# packages\n",
    "import pandas as pd\n",
    "from urllib.parse import urlparse,urlencode\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import whois\n",
    "import urllib.request\n",
    "from datetime import datetime\n",
    "import time\n",
    "import socket\n",
    "from urllib.error import HTTPError\n",
    "from cython.parallel import prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000997\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "itime = datetime.now()\n",
    "for i in prange(0,10000):\n",
    "    pass\n",
    "ftime = datetime.now()\n",
    "print(ftime-itime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "computer_raw_data=pd.read_csv(\"C:/Users/saiki/Desktop/Phishing-Website-Detection-50%/raw_datasets/100-legitimate-art.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rows = len(computer_raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#computer_raw_data[\"urls\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd = None\n",
    "class FeatureExtraction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def getProtocol(self,url):\n",
    "        return urlparse(url).scheme\n",
    "    \n",
    "    def getDomain(self,url):\n",
    "        return urlparse(url).netloc\n",
    "    \n",
    "    def getPath(self,url):\n",
    "        return urlparse(url).path\n",
    "    \n",
    "    def havingIP(self,url):\n",
    "        \"\"\"If the domain part has IP then it is phishing otherwise legitimate\"\"\"\n",
    "        match=re.search('(([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\.([01]?\\\\d\\\\d?|2[0-4]\\\\d|25[0-5])\\\\/)|'  #IPv4\n",
    "                    '((0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\.(0x[0-9a-fA-F]{1,2})\\\\/)'  #IPv4 in hexadecimal\n",
    "                    '(?:[a-fA-F0-9]{1,4}:){7}[a-fA-F0-9]{1,4}',url)     #Ipv6\n",
    "        if match:\n",
    "            #print match.group()\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            #print 'No matching pattern found'\n",
    "            return 0            # legitimate\n",
    "    \n",
    "    def long_url(self,url):\n",
    "        \"\"\"This function is defined in order to differntiate website based on the length of the URL\"\"\"\n",
    "        if len(url) < 54:\n",
    "            return 0            # legitimate\n",
    "        elif len(url) >= 54 and len(url) <= 75:\n",
    "            return 2            # suspicious\n",
    "        else:\n",
    "            return 1            # phishing\n",
    "    \n",
    "    def have_at_symbol(self,url):\n",
    "        \"\"\"This function is used to check whether the URL contains @ symbol or not\"\"\"\n",
    "        if \"@\" in url:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "    \n",
    "    def redirection(self,url):\n",
    "        \"\"\"If the url has symbol(//) after protocol then such URL is to be classified as phishing \"\"\"\n",
    "        if \"//\" in urlparse(url).path:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "        \n",
    "    def prefix_suffix_separation(self,url):\n",
    "        \"\"\"If the domain has '-' symbol then it is considered as phishing site\"\"\"\n",
    "        if \"-\" in urlparse(url).netloc:\n",
    "            return 1            # phishing\n",
    "        else:\n",
    "            return 0            # legitimate\n",
    "        \n",
    "    def sub_domains(self,url):\n",
    "        \"\"\"If the url has more than 3 dots then it is a phishing\"\"\"\n",
    "        if url.count(\".\") < 3:\n",
    "            return 0            # legitimate\n",
    "        elif url.count(\".\") == 3:\n",
    "            return 2            # suspicious\n",
    "        else:\n",
    "            return 1            # phishing\n",
    "        \n",
    "    def shortening_service(self,url):\n",
    "        \"\"\"Tiny URL -> phishing otherwise legitimate\"\"\"\n",
    "        match=re.search('bit\\.ly|goo\\.gl|shorte\\.st|go2l\\.ink|x\\.co|ow\\.ly|t\\.co|tinyurl|tr\\.im|is\\.gd|cli\\.gs|'\n",
    "                    'yfrog\\.com|migre\\.me|ff\\.im|tiny\\.cc|url4\\.eu|twit\\.ac|su\\.pr|twurl\\.nl|snipurl\\.com|'\n",
    "                    'short\\.to|BudURL\\.com|ping\\.fm|post\\.ly|Just\\.as|bkite\\.com|snipr\\.com|fic\\.kr|loopt\\.us|'\n",
    "                    'doiop\\.com|short\\.ie|kl\\.am|wp\\.me|rubyurl\\.com|om\\.ly|to\\.ly|bit\\.do|t\\.co|lnkd\\.in|'\n",
    "                    'db\\.tt|qr\\.ae|adf\\.ly|goo\\.gl|bitly\\.com|cur\\.lv|tinyurl\\.com|ow\\.ly|bit\\.ly|ity\\.im|'\n",
    "                    'q\\.gs|is\\.gd|po\\.st|bc\\.vc|twitthis\\.com|u\\.to|j\\.mp|buzurl\\.com|cutt\\.us|u\\.bb|yourls\\.org|'\n",
    "                    'x\\.co|prettylinkpro\\.com|scrnch\\.me|filoops\\.info|vzturl\\.com|qr\\.net|1url\\.com|tweez\\.me|v\\.gd|tr\\.im|link\\.zip\\.net',url)\n",
    "        if match:\n",
    "            return 1               # phishing\n",
    "        else:\n",
    "            return 0               # legitimate\n",
    "        \n",
    "    \"\"\"\n",
    "    def google_index(self,url):\n",
    "        user_agent = 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.116 Safari/537.36'\n",
    "        headers = { 'User-Agent' : user_agent}\n",
    "        query = {'q': 'info:' + url}\n",
    "        google = \"https://www.google.com/search?\" + urlencode(query)\n",
    "        #data = requests.get(google, headers=headers,proxies=proxies)\n",
    "        data = requests.get(google,headers=headers)\n",
    "        data.encoding = 'ISO-8859-1'\n",
    "        soup = BeautifulSoup(str(data.content), \"html.parser\")\n",
    "        try:\n",
    "            check = soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\")\n",
    "            if soup.find(id=\"rso\").find(\"div\").find(\"div\").find(\"h3\").find(\"a\").find(\"href\" != None):\n",
    "                href = check['href']\n",
    "                return 0 # indexed\n",
    "            else:\n",
    "                return 1\n",
    "        except AttributeError:\n",
    "            return 1 # indexed\n",
    "        #print(\"Waiting \" + str(seconds) + \" seconds until checking next URL.\\n\")\n",
    "        #time.sleep(float(seconds))\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def abnormal_url(self,url):\n",
    "        dns = 0\n",
    "        #domain_name = \"\"\n",
    "        try:\n",
    "            #domain = urlparse(url).netloc\n",
    "            #print(domain)\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "            #print(domain_name)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1 # phishing\n",
    "        else:\n",
    "            hostname=domain_name.domain_name\n",
    "            #match=re.search(hostname,url)\n",
    "            if hostname in url:\n",
    "                return 0 # legitimate\n",
    "            else:\n",
    "                return 1 # phishing\n",
    "    \"\"\"\n",
    "    \n",
    "    def web_traffic(self,url):\n",
    "        try:\n",
    "            rank = BeautifulSoup(urllib.request.urlopen(\"http://data.alexa.com/data?cli=10&dat=s&url=\" + url).read(), \"xml\").find(\"REACH\")['RANK']\n",
    "        except TypeError:\n",
    "            return 1\n",
    "        except HTTPError:\n",
    "            return 2\n",
    "        rank= int(rank)\n",
    "        if (rank<100000):\n",
    "            return 0\n",
    "        else:\n",
    "            return 2\n",
    "        \n",
    "    def domain_registration_length(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1      #phishing\n",
    "        else:\n",
    "            expiration_date = domain_name.expiration_date\n",
    "            today = time.strftime('%Y-%m-%d')\n",
    "            today = datetime.strptime(today, '%Y-%m-%d')\n",
    "            if expiration_date is None:\n",
    "                return 1\n",
    "            elif type(expiration_date) is list or type(today) is list :\n",
    "                return 2     #If it is a type of list then we can't select a single value from list. So,it is regarded as suspected website  \n",
    "            else:\n",
    "                creation_date = domain_name.creation_date\n",
    "                expiration_date = domain_name.expiration_date\n",
    "                if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "                    try:\n",
    "                        creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                        expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "                    except:\n",
    "                        return 2\n",
    "                registration_length = abs((expiration_date - today).days)\n",
    "                if registration_length / 365 <= 1:\n",
    "                    return 1 #phishing\n",
    "                else:\n",
    "                    return 0 # legitimate\n",
    "            \n",
    "    def age_domain(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            creation_date = domain_name.creation_date\n",
    "            expiration_date = domain_name.expiration_date\n",
    "            if (isinstance(creation_date,str) or isinstance(expiration_date,str)):\n",
    "                try:\n",
    "                    creation_date = datetime.strptime(creation_date,'%Y-%m-%d')\n",
    "                    expiration_date = datetime.strptime(expiration_date,\"%Y-%m-%d\")\n",
    "                except:\n",
    "                    return 2\n",
    "            if ((expiration_date is None) or (creation_date is None)):\n",
    "                return 1\n",
    "            elif ((type(expiration_date) is list) or (type(creation_date) is list)):\n",
    "                return 2\n",
    "            else:\n",
    "                ageofdomain = abs((expiration_date - creation_date).days)\n",
    "                if ((ageofdomain/30) < 6):\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "     \n",
    "    \n",
    "    def dns_record(self,url):\n",
    "        dns = 0\n",
    "        try:\n",
    "            domain_name = whois.whois(urlparse(url).netloc)\n",
    "            #rint(domain_name)\n",
    "        except:\n",
    "            dns = 1\n",
    "        \n",
    "        if dns == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "   \n",
    "    def statistical_report(self,url):\n",
    "        hostname = url\n",
    "        h = [(x.start(0), x.end(0)) for x in re.finditer('https://|http://|www.|https://www.|http://www.', hostname)]\n",
    "        z = int(len(h))\n",
    "        if z != 0:\n",
    "            y = h[0][1]\n",
    "            hostname = hostname[y:]\n",
    "            h = [(x.start(0), x.end(0)) for x in re.finditer('/', hostname)]\n",
    "            z = int(len(h))\n",
    "            if z != 0:\n",
    "                hostname = hostname[:h[0][0]]\n",
    "        url_match=re.search('at\\.ua|usa\\.cc|baltazarpresentes\\.com\\.br|pe\\.hu|esy\\.es|hol\\.es|sweddy\\.com|myjino\\.ru|96\\.lt|ow\\.ly',url)\n",
    "        try:\n",
    "            ip_address = socket.gethostbyname(hostname)\n",
    "            ip_match=re.search('146\\.112\\.61\\.108|213\\.174\\.157\\.151|121\\.50\\.168\\.88|192\\.185\\.217\\.116|78\\.46\\.211\\.158|181\\.174\\.165\\.13|46\\.242\\.145\\.103|121\\.50\\.168\\.40|83\\.125\\.22\\.219|46\\.242\\.145\\.98|107\\.151\\.148\\.44|107\\.151\\.148\\.107|64\\.70\\.19\\.203|199\\.184\\.144\\.27|107\\.151\\.148\\.108|107\\.151\\.148\\.109|119\\.28\\.52\\.61|54\\.83\\.43\\.69|52\\.69\\.166\\.231|216\\.58\\.192\\.225|118\\.184\\.25\\.86|67\\.208\\.74\\.71|23\\.253\\.126\\.58|104\\.239\\.157\\.210|175\\.126\\.123\\.219|141\\.8\\.224\\.221|10\\.10\\.10\\.10|43\\.229\\.108\\.32|103\\.232\\.215\\.140|69\\.172\\.201\\.153|216\\.218\\.185\\.162|54\\.225\\.104\\.146|103\\.243\\.24\\.98|199\\.59\\.243\\.120|31\\.170\\.160\\.61|213\\.19\\.128\\.77|62\\.113\\.226\\.131|208\\.100\\.26\\.234|195\\.16\\.127\\.102|195\\.16\\.127\\.157|34\\.196\\.13\\.28|103\\.224\\.212\\.222|172\\.217\\.4\\.225|54\\.72\\.9\\.51|192\\.64\\.147\\.141|198\\.200\\.56\\.183|23\\.253\\.164\\.103|52\\.48\\.191\\.26|52\\.214\\.197\\.72|87\\.98\\.255\\.18|209\\.99\\.17\\.27|216\\.38\\.62\\.18|104\\.130\\.124\\.96|47\\.89\\.58\\.141|78\\.46\\.211\\.158|54\\.86\\.225\\.156|54\\.82\\.156\\.19|37\\.157\\.192\\.102|204\\.11\\.56\\.48|110\\.34\\.231\\.42',ip_address)  \n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "        if url_match:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "        \n",
    "    def https_token(self,url):\n",
    "        match=re.search('https://|http://',url)\n",
    "        try:\n",
    "            if match.start(0)==0 and match.start(0) is not None:\n",
    "                url=url[match.end(0):]\n",
    "                match=re.search('http|https',url)\n",
    "                if match:\n",
    "                    return 1\n",
    "                else:\n",
    "                    return 0\n",
    "        except:\n",
    "            return 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features\n",
    "protocol = []\n",
    "domain = []\n",
    "path = []\n",
    "having_ip = []\n",
    "len_url = []\n",
    "having_at_symbol = []\n",
    "redirection_symbol = []\n",
    "prefix_suffix_separation = []\n",
    "sub_domains = []\n",
    "tiny_url = []\n",
    "abnormal_url = []\n",
    "web_traffic = []\n",
    "domain_registration_length = []\n",
    "dns_record = []\n",
    "statistical_report = []\n",
    "age_domain = []\n",
    "http_tokens = []\n",
    "#google_index = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0 - legitimate\n",
    "### 1 - phishing\n",
    "### 2 - suspicious"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'urls'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3628\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3629\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'urls'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10840\\1935363298.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# object creation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mfe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFeatureExtraction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcomputer_raw_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"urls\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3504\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3505\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3506\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3507\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3629\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcasted_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3630\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3631\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3632\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3633\u001b[0m                 \u001b[1;31m# If we have a listlike key, _check_indexing_error will raise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'urls'"
     ]
    }
   ],
   "source": [
    "# object creation\n",
    "fe = FeatureExtraction()\n",
    "rows = len(computer_raw_data[\"urls\"])\n",
    "\n",
    "for i in range(0,rows):\n",
    "    url=computer_raw_data[\"urls\"][i]\n",
    "    print(i ),print(url)\n",
    "    protocol.append(fe.getProtocol(url))\n",
    "    path.append(fe.getPath(url))\n",
    "    domain.append(fe.getDomain(url))\n",
    "    having_ip.append(fe.havingIP(url))\n",
    "    len_url.append(fe.long_url(url))\n",
    "    having_at_symbol.append(fe.have_at_symbol(url))\n",
    "    redirection_symbol.append(fe.redirection(url))\n",
    "    prefix_suffix_separation.append(fe.prefix_suffix_separation(url))\n",
    "    sub_domains.append(fe.sub_domains(url))\n",
    "    tiny_url.append(fe.shortening_service(url))\n",
    "    web_traffic.append(fe.web_traffic(url))\n",
    "    domain_registration_length.append(fe.domain_registration_length(url))\n",
    "    dns_record.append(fe.dns_record(url))\n",
    "    statistical_report.append(fe.statistical_report(url))\n",
    "    age_domain.append(fe.age_domain(url))\n",
    "    http_tokens.append(fe.https_token(url))\n",
    "    #google_index.append(fe.google_index(url))\n",
    "    #abnormal_url.append(fe.abnormal_url(url))\n",
    "    \n",
    "#print(domain)\n",
    "#print(protocol)\n",
    "#print(ip)\n",
    "#print(len_url)\n",
    "#print(google_index)\n",
    "#print(abnormal_url)\n",
    "#print(web_traffic)\n",
    "#print(domain_registration_length)\n",
    "#print(age_domain)\n",
    "#print(dns_record)\n",
    "#print(statistical_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for i in range(0,rows):\n",
    "    label.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d={'Protocol':pd.Series(protocol),'Domain':pd.Series(domain),'Path':pd.Series(path),'Having_IP':pd.Series(having_ip),\n",
    "   'URL_Length':pd.Series(len_url),'Having_@_symbol':pd.Series(having_at_symbol),\n",
    "   'Redirection_//_symbol':pd.Series(redirection_symbol),'Prefix_suffix_separation':pd.Series(prefix_suffix_separation),\n",
    "   'Sub_domains':pd.Series(sub_domains),'tiny_url':pd.Series(tiny_url),'web_traffic' : pd.Series(web_traffic) ,\n",
    "   'domain_registration_length':pd.Series(domain_registration_length),'dns_record':pd.Series(dns_record),\n",
    "   'statistical_report':pd.Series(statistical_report),'age_domain':pd.Series(age_domain),'http_tokens':pd.Series(http_tokens),\n",
    "   'label':pd.Series(label)}\n",
    "data=pd.DataFrame(d)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.to_csv(\"legitimate-urls.csv\",index=False,encoding='UTF-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv(\"phishing-urls.csv\",index=False,encoding='UTF-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
